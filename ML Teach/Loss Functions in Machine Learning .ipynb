{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference https://medium.com/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Categorical Crossentropy </h2>\n",
    "\n",
    "<p>\n",
    "This loss function work for multiclass, single-label classification. This employ when only one category applies to each data point. It compares the distribution of predictions (the activations in the output layer, one for each class) with the actual distribution, where the probability of the true class is 1and 0 for others. The true class is represented as a one-hot encoded vector, and the closer the model’s output to that vector, lower will be the Loss. In this category, the last layer activation function used is “Softmax” activation function.    \n",
    "    \n",
    "</p>\n",
    "\n",
    "<p> \n",
    "When to Use: In MNIST number digit classification problem, Categorical cross-entropy gives the probability that an image of a number is, a nine or a five or any different number.    \n",
    "</p>\n",
    "<img src=\"1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical_cross entropy loss is: 1.427065939827711\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = np.array([[0.25,0.25,0.25,0.25],\n",
    "                       [0.01,0.01,0.01,0.96]])\n",
    "\n",
    "# predictions = np.array([[0.0,0.0,0.0,0.9],\n",
    "#                         [0.01,0.01,0.0,0.96]])\n",
    "\n",
    "targets = np.array([[0,0,0,1],\n",
    "                   [0,0,0,1]])\n",
    "def categorical_crossentropy(predictions, targets, epsilon=1e-10):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce_loss = -np.sum(np.sum(targets * np.log(predictions + 1e-5)))\n",
    "    return ce_loss\n",
    "\n",
    "categorical_crossentropy_loss = categorical_crossentropy(predictions, targets)\n",
    "print (\"Categorical_cross entropy loss is: \" + str(categorical_crossentropy_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Binary Crossentropy </h2>\n",
    "\n",
    "<p> This loss function work for multiclass, multilabel classification. The Loss tells us how wrong the model predictions are. In multilabel problems, where an example can belong to multiple classes at the same time, the model tries to decide for each category whether the sample belongs to that category or not.\n",
    "Binary cross-entropy measures how far away from the actual value (either 0 or 1). The prediction is for each of the classes and then averages these class-wise errors to obtain the final Loss.\n",
    "</p>\n",
    "<p>When to use: If we want to determine the attitude of a piece of music. Every piece can have more than one attitude. For instance, it can be both “Happy” and “Energetic” at the same time. To solve this problem, we can use Binary cross-entropy. </p>\n",
    "\n",
    "\n",
    "<img src=\"2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary_Cross entropy loss is: 0.7135329699138555\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = np.array([[0.25,0.25,0.25,0.25],\n",
    "                        [0.01,0.01,0.01,0.96]])\n",
    "actual = np.array([[0,0,0,1],\n",
    "                   [0,0,0,1]])\n",
    "def binary_crossentropy(predictions, targets, epsilon=1e-10):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce_loss = -np.sum(np.sum(targets * np.log(predictions + 1e-5)))/N\n",
    "    return ce_loss\n",
    "binary_crossentropy_loss = binary_crossentropy(predictions, targets)\n",
    "print (\"Binary_Cross entropy loss is: \" + str(binary_crossentropy_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Mean Absolute Error(MAE) </h2>\n",
    "<p>It is the absolute difference between the actual and predicted value.\n",
    "MAE is not sensitive towards outliers and given several examples with the same input feature values, and the optimal prediction will be their median target value.\n",
    "The disadvantage of MAE is that the gradient magnitude is not dependent on the error size; it depends only on the sign of (y-y_hat). This means that the gradient magnitude will be considerable even when the error is small, which is, in turn, can lead to convergence problems.\n",
    "</p>\n",
    "<p>When to use: Use MAE where you are doing regression and don’t want outliers to play a significant role. It can also be useful if you know that your distribution is multimodel, and it’s desirable to have predictions in median mode.</p>\n",
    "<img src=\"3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p is: ['2.50000000', '0.00000000', '2.00000000', '8.00000000']\n",
      "a is: ['3.00000000', '-0.50000000', '2.00000000', '7.00000000']\n",
      "mae error is: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = np.array([2.5, 0.0, 2, 8])\n",
    "actual = np.array([3, -0.5, 2, 7])\n",
    "\n",
    "print(\"p is: \" + str([\"%.8f\" % elem for elem in predictions]))\n",
    "print(\"a is: \" + str([\"%.8f\" % elem for elem in actual]))\n",
    "\n",
    "def mae(predictions, actual):\n",
    "    difference = predictions - actual\n",
    "    absolute_difference = np.absolute(difference)\n",
    "    mean_absolute_difference = absolute_difference.mean()\n",
    "    return mean_absolute_difference\n",
    "mae_val = mae(predictions, actual)\n",
    "print (\"mae error is: \" + str(mae_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Mean Squared Error (MSE)</h2>\n",
    "<p>It is the mean of the squared difference between the actual and predicted value and most commonly used loss function for regression.\n",
    "MSE is sensitive towards outliers and given several examples with the same input feature values; the optimal prediction will be their mean target value.</p>\n",
    "<p>When to use: When doing regression. It penalizes significant error as compared to small ones.</p>\n",
    "\n",
    "<img src=\"4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p is: ['2.50000000', '0.00000000', '2.00000000', '8.00000000']\n",
      "a is: ['3.00000000', '-0.50000000', '2.00000000', '7.00000000']\n",
      "rms error is: 0.6123724356957945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictied = np.array([2.5, 0.0, 2, 8])\n",
    "actual = np.array([3, -0.5, 2, 7])\n",
    "def rmse(predicted, actual):\n",
    "    difference = predicted - actual\n",
    "    difference_squared = difference ** 2\n",
    "    mean_of_difference_squared = difference_squared.mean()\n",
    "    rmse_val = np.sqrt(mean_of_difference_squared)\n",
    "    return rmse_val\n",
    "print(\"p is: \" + str([\"%.8f\" % elem for elem in predictions]))\n",
    "print(\"a is: \" + str([\"%.8f\" % elem for elem in actual]))\n",
    "rmse_val = rmse(predictions, actual)\n",
    "print(\"rms error is: \" + str(rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
